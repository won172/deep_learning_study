{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 데이터에서 학습한다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가중치 매개변수의 값을 데이터를 보고 자동으로 결정한다는 뜻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 데이터 주도 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기계학습(머신러닝) : 사람의 개입을 최소화하고 수집한 데이터로부터 패턴을 찾으려 시도\\\n",
    "신경망, 딥러닝 : 기존 기계학습에서 사용하던 방법보다 사람의 개입을 더욱 배제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 훈련 데이터와 시험 데이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련 데이터와 시험 데이터를 나누는 이유 -> *범용능력을 평가하기 위해\\\n",
    "*범용능력이란? 아직 보지 못한 데이터로도 문제를 올바르게 풀어내는 능력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "오버피팅(과적합)을 피하는 것도 기계학습의 중요한 과제."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 손실 함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "신경망 학습에서는 현재의 상태를 '하나의 지표'로 표현함.\\\n",
    "그 지표를 가장 좋게 만들어주는 가중치 매개변수의 값을 탐색하는 것.\\\n",
    "-> 신경망 학습에서 사용하는 지표 : 손실함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 오차제곱합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 회귀에서 많이 씀."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0] # 소프트맥스 함수의 출력값\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0] # one-hot encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 배열은 첫 번째 인덱스부터 순서대로 숫자 '0', '1', '2', ...일 때의 값임.\\\n",
    "y는 해당 숫자일 확률, t는 해당 숫자일 때 1을 나타냄."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오차제곱합 구현\n",
    "def sum_squares_error(y, t):\n",
    "    return 0.5 * np.sum((y-t)**2) # ((각 원소의 출력 - 정답 레이블)**2) 총합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5를 왜 곱할까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답은 '2'\n",
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.09750000000000003)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예1 : '2'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5975)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 예2 : '7'일 확률이 가장 높다고 추정함(0.6)\n",
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "sum_squares_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과\n",
    "- 첫 번째 예의 손실 함수 쪽 출력이 작으며 정답 레이블과의 오차도 작은 것을 알 수 있음.\n",
    "- 즉, 오차제곱합 기준으로는 첫 번째 추정 결과가 정답에 더 가까울 것으로 판단할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 교차 엔트로피 오차"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보통 분류에서 많이 씀."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정답일 때의 출력이 전체 값을 정하게 됨.\\\n",
    "정답일 때의 출력이 작아질수록 오차는 커짐."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_error(y, t):\n",
    "    delta = 1e-7\n",
    "    return -np.sum(t * np.log(y+delta)) # np.log()에 0이 입력되면 -inf가 되어 계산할 수 없기 때문에 delta를 더해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.510825457099338)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "y = [0.1, 0.05, 0.6, 0.0, 0.05, 0.1, 0.0, 0.1, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(2.302584092994546)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [0.1, 0.05, 0.1, 0.0, 0.05, 0.1, 0.0, 0.6, 0.0, 0.0]\n",
    "cross_entropy_error(np.array(y), np.array(t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "결과\n",
    "- 결과(오차 값)가 더 작은 첫 번째 추정이 정답일 가능성이 높다고 판단.\n",
    "- 정답일 가능성이 놏아질수록 오차 값이 줄어듦."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 미니배치 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니배치 학습을 하는 이유!\n",
    "- 훈련 데이터가 매우 크면 모든 데이터를 대상으로 손실 함수의 하을 구하려면 시간이 오래 걸림.\n",
    "- 많은 데이터를 대상으로 손실 함수를 계산하는 것은 현실적이지 않음.\\\n",
    "-> 데이터 일부를 추려 전체의 '근사치'로 이용.\\\n",
    "-> 훈련 데이터로부터 일부만 골라 학습 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(60000, 10)\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from datasets.mnist import load_mnist\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "print(x_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = x_train.shape[0]\n",
    "batch_size = 10\n",
    "batch_mask = np.random.choice(train_size, batch_size)\n",
    "x_batch = x_train[batch_mask]\n",
    "t_batch = t_train[batch_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([36319,  2572, 42345,  3058, 12664,  4456, 52147, 43015, 11554,\n",
       "       46505], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.choice(60000, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 (배치용) 교차 엔트로피 오차 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 원-핫 인코딩인 경우\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(t * np.log(y + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정답 레이블이 숫자 레이블인 경우\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "\n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.5 왜 손실 함수를 설정하는가?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'정확도'대신 '손실 함수'를 사용하는 이유는?\n",
    "- 신경망 학습에서는 최적의 매개변수를 탐색할 때 손실함수의 값을 가능한 한 작게 하는 매개변수 값을 찾음.\n",
    "- 매개변수의 미분을 계산하고, 그 미분 값을 단서로 매개변수의 값을 갱신하는 과정 반복\n",
    "- 미분 값이 음수면? -> 가중치 매개변수를 양의 방향으로 변화시켜 손실 함수의 값을 줄임.\n",
    "- 미분 값이 양수면? -> 가중치 매개변수를 음의 방향으로 변화시켜 손실 함수의 값을 줄임.\n",
    "- 미분 값이 0이면? -> 매개변수를 어느 쪽으로 움직여도 손실 함수의 값은 줄어들지 않음. -> 이 때 갱신을 멈춤.\\\n",
    "\n",
    "정확도를 지표로 삼아서는 안 되는 이유는?\n",
    "- 미분 값이 대부분의 장소에서 0이 되어 매개변수를 갱신할 수 없기 때문\n",
    "- 매개변수의 미소한 변화에는 거의 반응을 보이지 않음.\n",
    "- 반응이 있더라도 그 값이 불연속적으로 갑자기 변화."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 수치 미분"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나쁜 구현\n",
    "def numerical_diff(f, x):\n",
    "    h = 10e-50\n",
    "    return (f(x+h) - f(x)) / h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float32(0.0)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.float32(1e-50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_diff(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    return (f(x+h) - f(x-h)) / (2*h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.2 수치 미분의 예"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_1(x):\n",
    "    return 0.01*x**2 + 0.1*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.arange(0.0, 20.0, 0.1) # 0에서 20까지 0.1 간격의 배열 x를 만든다.\n",
    "y = function_1(x)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1999999999990898"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2999999999986347"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_1, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.3 편미분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2\n",
    "    # 또는 return np.sum(x**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp1(x0):\n",
    "    return x0*x0 + 4.0**2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.00000000000378"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp1, 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "문제 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_tmp2(x1):\n",
    "    return 3.0**2.0 + x1*x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.999999999999119"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_diff(function_tmp2, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x) # x와 형상이 같은 배열을 생성\n",
    "\n",
    "    for idx in range(x.size):\n",
    "        tmp_val = x[idx]\n",
    "        # f(x+h) 계산\n",
    "        x[idx] = tmp_val + h\n",
    "        fxh1 = f(x)\n",
    "\n",
    "        # f(x-h) 계산\n",
    "        x[idx] = tmp_val - h\n",
    "        fxh2 = f(x)\n",
    "\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        x[idx] = tmp_val # 값 복원\n",
    "\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 8.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 4.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 4.])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([0.0, 2.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 0.])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerical_gradient(function_2, np.array([3.0, 0.0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, init_x, lr=0.01, step_num=100):\n",
    "    x = init_x\n",
    "\n",
    "    for i in range(step_num):\n",
    "        grad = numerical_gradient(f, x)\n",
    "        x -= lr * grad\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function_2(x):\n",
    "    return x[0]**2 + x[1]**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.11110793e-10,  8.14814391e-10])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=0.1, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.58983747e+13, -1.29524862e+12])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 큰 예 : lr=10.0\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=10.0, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.99999994,  3.99999992])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 학습률이 너무 작은 예 : lr=1e-10\n",
    "init_x = np.array([-3.0, 4.0])\n",
    "gradient_descent(function_2, init_x=init_x, lr=1e-10, step_num=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "def softmax(x):\n",
    "    x = x - np.max(x, axis=-1, keepdims=True) # 오버플로 대책\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=-1, keepdims=True)\n",
    "def cross_entropy_error(y, t):\n",
    "    if y.ndim == 1:\n",
    "        t = t.reshape(1, t.size)\n",
    "        y = y.reshape(1, y.size)\n",
    "        \n",
    "    # 훈련 데이터가 원-핫 벡터라면 정답 레이블의 인덱스로 반환\n",
    "    if t.size == y.size:\n",
    "        t = t.argmax(axis=1)\n",
    "             \n",
    "    batch_size = y.shape[0]\n",
    "    return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size\n",
    "\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "class simpleNet:\n",
    "    def __init__(self):\n",
    "        self.W = np.random.randn(2, 3) # 정규분포로 초기화\n",
    "\n",
    "    def predict(self, x):\n",
    "        return np.dot(x, self.W)\n",
    "\n",
    "    def loss(self, x, t):\n",
    "        z = self.predict(x)\n",
    "        y = softmax(z)\n",
    "        loss = cross_entropy_error(y, t)\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.83734509 -0.76661018  0.13439913]\n",
      " [ 0.0127889  -0.22697866  1.20174519]]\n"
     ]
    }
   ],
   "source": [
    "net = simpleNet()\n",
    "print(net.W) # 가중치 매개변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.51391707 -0.6642469   1.16221014]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([0.6, 0.9])\n",
    "p = net.predict(x)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(p) # 최댓값의 인덱스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.5211245632248231)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = np.array([0, 0, 1]) # 정답 레이블\n",
    "net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(W):\n",
    "    return net.loss(x, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.18632858  0.05736003 -0.24368861]\n",
      " [ 0.27949287  0.08604004 -0.36553292]]\n"
     ]
    }
   ],
   "source": [
    "dW = numerical_gradient(f, net.W) # (손실함수, 가중치 매개변수)\n",
    "print(dW) # 기울기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = lambda w: net.loss(x, t)\n",
    "dW = numerical_gradient(f, net.W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 학습 알고리즘 구현하기기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**전제**\n",
    "\n",
    "- 신경망에는 적응 가능한 가중치와 편향이 있고, 이 가중치와 편향을 훈련 데이터에 조정하는 과정을 학습이라고 한다.\n",
    "\n",
    "**1단계: 미니배치**\n",
    "\n",
    "- **훈련 데이터 중 일부를 무작위로 가져온다. 이렇게 선별한 데이터를 미니배치라고 하며, 그 미니배치의 손실함수 값을 줄이는 것이 목표**이다.\n",
    "\n",
    "**2단계: 기울기 산출**\n",
    "\n",
    "- 미니배치의 손실 함수 값을 줄이기 위해 각 가중치 매개변수의 기울기를 구한다. \n",
    "\n",
    "- 기울기는 손실 함수의 값을 가장 작게 하는 방향을 제시한다.\n",
    "\n",
    "**3단계: 매개변수 갱신**\n",
    "\n",
    "- 가중치 매개변수를 기울기 방향으로 아주 조금 갱신한다\n",
    "\n",
    "**4단계: 반복**\n",
    "\n",
    "- 1~3단계를 반복한다"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1 2층 신경망 클래스 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(os.pardir)\n",
    "from common.functions import *\n",
    "def numerical_gradient(f, x):\n",
    "    h = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + h\n",
    "        fxh1 = f(x) # f(x+h)\n",
    "        \n",
    "        x[idx] = tmp_val - h \n",
    "        fxh2 = f(x) # f(x-h)\n",
    "        grad[idx] = (fxh1 - fxh2) / (2*h)\n",
    "        \n",
    "        x[idx] = tmp_val # 값 복원\n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "class TwoLayerNet:\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_size, weight_init_std=0.01):\n",
    "        # 가중치 초기화\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(input_size, hidden_size)\n",
    "        self.params['b1'] = np.zeros(hidden_size)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b2'] = np.zeros(output_size)\n",
    "\n",
    "    def predict(self, x):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "    \n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        return y\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        \n",
    "        return cross_entropy_error(y, t)\n",
    "    \n",
    "    def accuracy(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        y = np.argmax(y, axis=1)\n",
    "        t = np.argmax(t, axis=1)\n",
    "        \n",
    "        accuracy = np.sum(y == t) / float(x.shape[0])\n",
    "        return accuracy\n",
    "        \n",
    "    # x : 입력 데이터, t : 정답 레이블\n",
    "    def numerical_gradient(self, x, t):\n",
    "        loss_W = lambda W: self.loss(x, t)\n",
    "        \n",
    "        grads = {}\n",
    "        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n",
    "        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n",
    "        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n",
    "        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n",
    "        \n",
    "        return grads\n",
    "        \n",
    "    def gradient(self, x, t):\n",
    "        W1, W2 = self.params['W1'], self.params['W2']\n",
    "        b1, b2 = self.params['b1'], self.params['b2']\n",
    "        grads = {}\n",
    "        \n",
    "        batch_num = x.shape[0]\n",
    "        \n",
    "        # forward\n",
    "        a1 = np.dot(x, W1) + b1\n",
    "        z1 = sigmoid(a1)\n",
    "        a2 = np.dot(z1, W2) + b2\n",
    "        y = softmax(a2)\n",
    "        \n",
    "        # backward\n",
    "        dy = (y - t) / batch_num\n",
    "        grads['W2'] = np.dot(z1.T, dy)\n",
    "        grads['b2'] = np.sum(dy, axis=0)\n",
    "        \n",
    "        da1 = np.dot(dy, W2.T)\n",
    "        dz1 = sigmoid_grad(a1) * da1\n",
    "        grads['W1'] = np.dot(x.T, dz1)\n",
    "        grads['b1'] = np.sum(dz1, axis=0)\n",
    "\n",
    "        return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "net = TwoLayerNet(input_size=784, hidden_size = 100, output_size=10)\n",
    "print(net.params['W1'].shape)\n",
    "print(net.params['b1'].shape)\n",
    "print(net.params['W2'].shape)\n",
    "print(net.params['b2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.rand(100, 784)\n",
    "y = net.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 100)\n",
      "(100,)\n",
      "(100, 10)\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "x = np.random.rand(100, 784) # 더미 입력 데이터(100장 분량)\n",
    "t = np.random.rand(100, 10) # 더미 정답 데이터(100장 분량)\n",
    "\n",
    "grads = net.numerical_gradient(x, t)\n",
    "\n",
    "print(grads['W1'].shape)\n",
    "print(grads['b1'].shape)\n",
    "print(grads['W2'].shape)\n",
    "print(grads['b2'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.2 미니배치 학습 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'key_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[52], line 44\u001b[0m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_img\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_label\u001b[39m\u001b[38;5;124m'\u001b[39m]), (dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_img\u001b[39m\u001b[38;5;124m'\u001b[39m], dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_label\u001b[39m\u001b[38;5;124m'\u001b[39m]) \n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 44\u001b[0m     init_mnist()\n\u001b[0;32m     46\u001b[0m (x_train, t_train), (x_test, t_test) \u001b[38;5;241m=\u001b[39m load_mnist(normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, one_hot_label\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     48\u001b[0m train_loss_list \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[1;32mIn[50], line 3\u001b[0m, in \u001b[0;36minit_mnist\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minit_mnist\u001b[39m():\n\u001b[1;32m----> 3\u001b[0m     download_mnist()\n\u001b[0;32m      4\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m _convert_numpy()\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating pickle file ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[52], line 3\u001b[0m, in \u001b[0;36mdownload_mnist\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdownload_mnist\u001b[39m():\n\u001b[1;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m key_file\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[0;32m      4\u001b[0m        _download(v)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'key_file' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def download_mnist():\n",
    "    for v in key_file.values():\n",
    "       _download(v)\n",
    "       \n",
    "def load_mnist(normalize=True, flatten=True, one_hot_label=False):\n",
    "    \"\"\"MNIST 데이터셋 읽기\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalize : 이미지의 픽셀 값을 0.0~1.0 사이의 값으로 정규화할지 정한다.\n",
    "    one_hot_label : \n",
    "        one_hot_label이 True면、레이블을 원-핫(one-hot) 배열로 돌려준다.\n",
    "        one-hot 배열은 예를 들어 [0,0,1,0,0,0,0,0,0,0]처럼 한 원소만 1인 배열이다.\n",
    "    flatten : 입력 이미지를 1차원 배열로 만들지를 정한다. \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (훈련 이미지, 훈련 레이블), (시험 이미지, 시험 레이블)\n",
    "    \"\"\"\n",
    "    if not os.path.exists(save_file):\n",
    "        init_mnist()\n",
    "        \n",
    "    with open(save_file, 'rb') as f:\n",
    "        dataset = pickle.load(f)\n",
    "    \n",
    "    if normalize:\n",
    "        for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].astype(np.float32)\n",
    "            dataset[key] /= 255.0\n",
    "            \n",
    "    if one_hot_label:\n",
    "        dataset['train_label'] = _change_one_hot_label(dataset['train_label'])\n",
    "        dataset['test_label'] = _change_one_hot_label(dataset['test_label'])    \n",
    "    \n",
    "    if not flatten:\n",
    "         for key in ('train_img', 'test_img'):\n",
    "            dataset[key] = dataset[key].reshape(-1, 1, 28, 28)\n",
    "\n",
    "    return (dataset['train_img'], dataset['train_label']), (dataset['test_img'], dataset['test_label']) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    init_mnist()\n",
    "\n",
    "(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)\n",
    "\n",
    "train_loss_list = []\n",
    "\n",
    "# 하이퍼파라미터\n",
    "iters_num = 10000 # 반복 횟수\n",
    "train_size = x_train.shape[0]\n",
    "batch_size = 100 # 미니배치 크기\n",
    "learning_rate = 0.1\n",
    "network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\n",
    "\n",
    "for i in range(iters_num):\n",
    "    # 미니배치 획득\n",
    "    batch_mask = np.random.choice(train_size, batch_size)\n",
    "    x_batch = x_train[batch_mask]\n",
    "    t_batch = t_train[batch_mask]\n",
    "    \n",
    "    # 기울기 계산\n",
    "    grad = network.numerical_gradient(x_batch, t_batch)\n",
    "    # grad = network.gradient(x_batch, t_batch) # 성능 개선판\n",
    "    \n",
    "    # 매개변수 갱신\n",
    "    for key in ('W1', 'b1', 'W2', 'b2'):\n",
    "        network.params[key] -= learning_rate * grad[key]\n",
    "        \n",
    "    # 학습 경과 기록\n",
    "    loss = network.loss(x_batch, t_batch)\n",
    "    train_loss_list.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BAF_kernel",
   "language": "python",
   "name": "baf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
